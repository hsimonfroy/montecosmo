{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "Infer from a cosmological model via MCMC samplers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 22:31:49.864196: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 11.5 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='.6' # NOTE: jax preallocates GPU (default 75%)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import numpy as jnp, random as jr, jit, vmap, grad, debug, tree\n",
    "\n",
    "from functools import partial\n",
    "from getdist import plots\n",
    "from numpyro import infer\n",
    "\n",
    "from montecosmo.model import FieldLevelModel, default_config\n",
    "from montecosmo.utils import pdump, pload\n",
    "from montecosmo.mcbench import sample_and_save\n",
    "from montecosmo.script import from_id, get_mcmc, get_init_mcmc\n",
    "\n",
    "# import mlflow\n",
    "# mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8081\")\n",
    "# mlflow.set_experiment(\"infer\")\n",
    "# !jupyter nbconvert --to script ./src/montecosmo/tests/infer_model.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and fiduc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_ARRAY_TASK_ID: 3130\n"
     ]
    }
   ],
   "source": [
    "################## TO SET #######################\n",
    "# task_id = int(os.environ['SLURM_ARRAY_TASK_ID'])\n",
    "task_id = 3130\n",
    "print(\"SLURM_ARRAY_TASK_ID:\", task_id)\n",
    "model, mcmc_config, save_dir, save_path = from_id(task_id)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# import sys\n",
    "# tempstdout, tempstderr = sys.stdout, sys.stderr\n",
    "# sys.stdout = sys.stderr = open(save_path+'.out', 'a')\n",
    "# job_id = int(os.environ['SLURM_ARRAY_JOB_ID'])\n",
    "# print(\"SLURM_ARRAY_JOB_ID:\", job_id)\n",
    "# print(\"SLURM_ARRAY_TASK_ID:\", task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CONFIG\n",
      "{'a_lpt': 0.5,\n",
      " 'a_obs': 0.5,\n",
      " 'box_shape': array([650., 650., 650.]),\n",
      " 'gxy_density': 0.001,\n",
      " 'latents': {'Omega_m': {'group': 'cosmo',\n",
      "                         'high': 1.0,\n",
      "                         'label': '{\\\\Omega}_m',\n",
      "                         'loc': 0.3111,\n",
      "                         'low': 0.05,\n",
      "                         'scale': 0.2},\n",
      "             'b1': {'group': 'bias',\n",
      "                    'label': '{b}_1',\n",
      "                    'loc': 1.0,\n",
      "                    'scale': 0.5},\n",
      "             'b2': {'group': 'bias',\n",
      "                    'label': '{b}_2',\n",
      "                    'loc': 0.0,\n",
      "                    'scale': 2.0},\n",
      "             'bn2': {'group': 'bias',\n",
      "                     'label': '{b}_{\\\\nabla^2}',\n",
      "                     'loc': 0.0,\n",
      "                     'scale': 2.0},\n",
      "             'bs2': {'group': 'bias',\n",
      "                     'label': '{b}_{s^2}',\n",
      "                     'loc': 0.0,\n",
      "                     'scale': 2.0},\n",
      "             'init_mesh': {'group': 'init',\n",
      "                           'label': '{\\\\delta}_L'},\n",
      "             'sigma8': {'group': 'cosmo',\n",
      "                        'label': '{\\\\sigma}_8',\n",
      "                        'loc': 0.8102,\n",
      "                        'low': 0.0,\n",
      "                        'scale': 0.2}},\n",
      " 'lpt_order': 1,\n",
      " 'mesh_shape': array([130, 130, 130]),\n",
      " 'nbody_steps': 5,\n",
      " 'obs': 'mesh',\n",
      " 'precond': 3,\n",
      " 'snapshots': None}\n",
      "\n",
      "# INFOS\n",
      "cell_shape:     [5.0, 5.0, 5.0] Mpc/h\n",
      "k_funda:        0.00967 h/Mpc\n",
      "k_nyquist:      0.62832 h/Mpc\n",
      "mean_gxy_count: 0.125 gxy/cell\n",
      "\n",
      "{'sampler': 'NUTS', 'target_accept_prob': 0.65, 'n_samples': 64, 'max_tree_depth': 10, 'n_runs': 10, 'n_chains': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and truth at /feynman/home/dphp/hs276503/scratch/pickles/m130_b650.0_al0.5_ao0.5_lo1_pc3_obmesh/\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(mcmc_config)\n",
    "# model.render()\n",
    "\n",
    "if not os.path.exists(save_dir+\"truth.p\") or True:\n",
    "    # Predict and save fiducial\n",
    "    truth = {'Omega_m': 0.31, \n",
    "            'sigma8': 0.81, \n",
    "            'b1': 1., \n",
    "            'b2':0., \n",
    "            'bs2':0., \n",
    "            'bn2': 0.}\n",
    "\n",
    "    model.reset()\n",
    "    truth = model.predict(samples=truth, hide_base=False, hide_samp=False, frombase=True)\n",
    "    \n",
    "    print(f\"Saving model and truth at {save_dir}\")\n",
    "    model.save(save_dir)    \n",
    "    pdump(truth, save_dir+\"truth.p\")\n",
    "else:\n",
    "    print(f\"Loading truth from {save_dir}\")\n",
    "    truth = pload(save_dir+\"truth.p\")\n",
    "\n",
    "model.condition({'obs': truth['obs']})\n",
    "model.delta_obs = truth['obs'] - 1\n",
    "model.block()\n",
    "# model.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS, HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries.optimizers import adam\n",
    "from tqdm import tqdm\n",
    "from jax import value_and_grad\n",
    "\n",
    "def optimize(potential, start, lr0=0.1, n_epochs=100):\n",
    "    energies = []\n",
    "\n",
    "    lr_fn = lambda i: lr0 / (1 + i)**.5\n",
    "    opt_init, opt_update, get_params = adam(lr_fn)\n",
    "    opt_state = opt_init(start)\n",
    "\n",
    "    @jit\n",
    "    def step(step, opt_state):\n",
    "        value, grads = value_and_grad(potential)(get_params(opt_state))\n",
    "        opt_state = opt_update(step, grads, opt_state)\n",
    "        return value, opt_state\n",
    "\n",
    "    for i_epoch in tqdm(range(n_epochs)):\n",
    "        value, opt_state = step(i_epoch, opt_state)\n",
    "        energies.append(value.astype(float))\n",
    "    params = get_params(opt_state)\n",
    "    return params, energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Warmupping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run 0/0 (warmup)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      "/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]/feynman/work/dphp/hs276503/envs/montenvtest3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n",
      "warmup:   3%|â–Ž         | 1/32 [02:07<1:05:37, 127.00s/it]"
     ]
    }
   ],
   "source": [
    "continue_run = False\n",
    "optim = False\n",
    "if continue_run:\n",
    "    model.reset()\n",
    "    model.condition({'obs': truth['obs']})\n",
    "    model.block()\n",
    "    mcmc = get_mcmc(model.model, mcmc_config)\n",
    "\n",
    "    last_state = pload(save_path + \"_last_state.p\")\n",
    "    mcmc.num_warmup = 0\n",
    "    mcmc.post_warmup_state = last_state\n",
    "    init_params_ = None\n",
    "else:\n",
    "    model.reset()\n",
    "    model.condition({'obs': truth['obs']} | model.prior_loc, frombase=True)\n",
    "    model.block()\n",
    "\n",
    "    mcmc = get_init_mcmc(model.model, mcmc_config['n_chains'])    \n",
    "    print(\"# Warmupping...\")\n",
    "    init_params_ = jit(vmap(model.init_model))(jr.split(jr.key(43), mcmc.num_chains))\n",
    "    # init_params_ = model.predict(45, samples=mcmc.num_chains, hide_samp=False)\n",
    "\n",
    "    init_mesh_ = {k: init_params_[k] for k in ['init_mesh_']} # NOTE: !!!!!!!\n",
    "    if optim==False:\n",
    "        mcmc = sample_and_save(mcmc, save_path+'_init', 0, 0, extra_fields=['num_steps'], init_params=init_mesh_)\n",
    "        ils = mcmc.last_state.z\n",
    "    else:\n",
    "        warmup_params, energies = vmap(partial(optimize, model.potential, lr0=1., n_epochs=100))(init_mesh_)\n",
    "        print(warmup_params.keys(), jnp.shape(energies))\n",
    "        ils = warmup_params\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(energies)\n",
    "        plt.xlabel(\"Epoch\"), plt.ylabel(\"Potential\")\n",
    "        # plt.savefig(save_dir+f'optimpk_{task_id}.png')\n",
    "        plt.savefig(f'optimpk_{task_id}.png')\n",
    "        plt.show()\n",
    "\n",
    "    ################\n",
    "    from montecosmo.plot import plot_pow, plot_powtranscoh, plot_coh\n",
    "    mesh0 = jnp.fft.irfftn(truth['init_mesh'])\n",
    "    kptcs__ = vmap(lambda x: model.powtranscoh(mesh0, model.reparam(x, fourier=False)['init_mesh']))(init_params_)\n",
    "    kptcs_ = vmap(lambda x: model.powtranscoh(mesh0, model.reparam(x, fourier=False)['init_mesh']))(init_params_ | ils)\n",
    "    kpk0 = model.spectrum(mesh0)\n",
    "    kptc_obs = model.powtranscoh(mesh0, truth['obs'] - 1)\n",
    "    kpkobs = model.spectrum(truth['obs']-1)\n",
    "    \n",
    "    print(ils.keys(), init_params_.keys())\n",
    "\n",
    "    mse__ = jnp.mean((vmap(lambda x: model.reparam(x, fourier=False))(init_params_)['init_mesh']  - mesh0)**2, axis=(1,2,3))\n",
    "    mse_ = jnp.mean((vmap(lambda x: model.reparam(x, fourier=False))(init_params_ | ils)['init_mesh']  - mesh0)**2, axis=(1,2,3))\n",
    "    print(\"MSEs:\", mse_, mse_)\n",
    "\n",
    "    prob = 0.95\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plot_powtranscoh(*jnp.median(jnp.stack(kptcs__), 1), label='init')\n",
    "    plot_powtranscoh(*kptcs__, fill=prob)\n",
    "    plot_powtranscoh(*jnp.median(jnp.stack(kptcs_), 1), label='warm')\n",
    "    plot_powtranscoh(*kptcs_, fill=prob)\n",
    "    plt.subplot(131)\n",
    "    plot_pow(*kpk0, 'k', label='true')\n",
    "    plot_pow(*kpkobs, ':', c='grey', label='obs')\n",
    "    plt.legend()\n",
    "    plt.subplot(133)\n",
    "    plot_coh(kptc_obs[0], kptc_obs[-1], ':', c='grey', label='obs')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(save_dir+f'initpk_{task_id}.png')\n",
    "    plt.savefig(f'init_glin_126_{task_id}.png')\n",
    "\n",
    "    if optim==False:\n",
    "        print(\"mean_acc_prob:\", mcmc.last_state.mean_accept_prob, \n",
    "            \"\\nss:\", mcmc.last_state.adapt_state.step_size, \n",
    "            \"\\nmm_sqrt:\", mcmc.last_state.adapt_state.mass_matrix_sqrt)\n",
    "    ################\n",
    "    \n",
    "    init_params_ |= ils\n",
    "    # init_params_ |= mcmc.last_state.z\n",
    "    print(init_params_.keys())\n",
    "    model.reset()\n",
    "    model.condition({'obs': truth['obs']})\n",
    "    model.block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mcmc_config['sampler'] != 'NUTSwG':\n",
    "    mcmc = get_mcmc(model.model, mcmc_config)\n",
    "    mcmc_runned = sample_and_save(mcmc, save_path, 0, mcmc_config['n_runs'], extra_fields=['num_steps'], init_params=init_params_)\n",
    "\n",
    "else:\n",
    "    from montecosmo.samplers import NUTSwG_init, get_NUTSwG_warm, get_NUTSwG_run\n",
    "\n",
    "    step_fn, init_fn, parameters, init_state_fn = NUTSwG_init(model.logpdf)\n",
    "    warmup_fn = jit(vmap(get_NUTSwG_warm(model.logpdf, parameters, mcmc_config['n_samples'])))\n",
    "    last_state = jit(vmap(init_state_fn))(init_params_)\n",
    "    # last_state = pload(save_dir+\"NUTSGibbs/HMCGibbs_ns256_x_nc8_laststate32.p\")\n",
    "\n",
    "\n",
    "    (last_state, parameters), samples, infos = warmup_fn(jr.split(jr.key(43), mcmc_config['n_chains']), last_state)\n",
    "    print(\"parameters:\", parameters,\n",
    "            \"infos:\", infos, '\\n#################\\n')\n",
    "    jnp.savez(save_path+f\"_{0}.npz\", **samples | {k:infos[k] for k in ['n_evals']})\n",
    "    pdump(last_state, save_path+f\"_last_state.p\")\n",
    "    # pdump(parameters, save_path+'_parameters.p'), pdump(tree.map(jnp.mean, infos), save_path+'_infos.p')\n",
    "\n",
    "    run_fn = jit(vmap(get_NUTSwG_run(model.logpdf, step_fn, init_fn, mcmc_config['n_samples'])))\n",
    "    start = 1\n",
    "    end = start + mcmc_config['n_runs']-1\n",
    "    key = jr.key(42)\n",
    "    for i_run in range(start, end+1):\n",
    "        print(f\"run {i_run}/{end}\")\n",
    "        key, run_key = jr.split(key, 2)\n",
    "        last_state, samples, infos = run_fn(jr.split(run_key, mcmc_config['n_chains']), last_state, parameters=parameters)\n",
    "        print(\"infos:\", infos)\n",
    "        jnp.savez(save_path+f\"_{i_run}.npz\", **samples | {k:infos[k] for k in ['n_evals']})\n",
    "        pdump(last_state, save_path+f\"_last_state.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.reset()\n",
    "# model.condition({'obs': truth['obs']})\n",
    "# model.block()\n",
    "# mcmc = get_mcmc(model.model, mcmc_config)\n",
    "# init_params_ = {k+'_': jnp.broadcast_to(truth[k+'_'], (mcmc_config['n_chains'], *jnp.shape(truth[k+'_']))) for k in ['Omega_m','sigma8','b1','b2','bs2','bn2','init_mesh']}\n",
    "\n",
    "# mcmc_runned = sample_and_save(mcmc, mcmc_config['n_runs'], save_path, extra_fields=['num_steps'], init_params=init_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
